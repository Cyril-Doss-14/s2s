{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb21174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.13.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl (276.6 MB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading protobuf-4.24.1-cp310-abi3-win_amd64.whl (430 kB)\n",
      "                                              0.0/430.4 kB ? eta -:--:--\n",
      "     --------                                92.2/430.4 kB 2.6 MB/s eta 0:00:01\n",
      "     ---------------                        174.1/430.4 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------              286.7/430.4 kB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------------------  430.1/430.4 kB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 430.4/430.4 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (67.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached grpcio-1.57.0-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.29.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.16)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: libclang, flatbuffers, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.57.0 keras-2.13.1 libclang-16.0.6 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.24.1 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 typing-extensions-4.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\cyril\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\cyril\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\cyril\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\cyril\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n",
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca5d363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3425 images belonging to 36 classes.\n",
      "Found 2026 images belonging to 36 classes.\n",
      "Epoch 1/5\n",
      "85/85 [==============================] - 70s 771ms/step - loss: 1.4291 - accuracy: 0.6901 - val_loss: 0.0723 - val_accuracy: 0.9955\n",
      "Epoch 2/5\n",
      "85/85 [==============================] - 40s 474ms/step - loss: 0.0311 - accuracy: 0.9997 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "85/85 [==============================] - 41s 488ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "85/85 [==============================] - 39s 462ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "85/85 [==============================] - 35s 414ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Bidirectional, LSTM, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define function to split data into train and val directories\n",
    "def split_data(source, train, val, split_size):\n",
    "    files = os.listdir(source)\n",
    "    random.shuffle(files)\n",
    "    split_index = int(split_size * len(files))\n",
    "    train_files = files[:split_index]\n",
    "    val_files = files[split_index:]\n",
    "\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(source, file), os.path.join(train, file))\n",
    "\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(source, file), os.path.join(val, file))\n",
    "\n",
    "# Define function to process images\n",
    "def process_images(from_dir, to_dir):\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "\n",
    "    # Process images and copy them to the target directory\n",
    "    for file in os.listdir(from_dir):\n",
    "        hand_img = cv2.imread(os.path.join(from_dir, file))\n",
    "        if hand_img is not None:\n",
    "            hand_img = cv2.resize(hand_img, (100, 100))  # Resize to smaller image size\n",
    "            cv2.imwrite(os.path.join(to_dir, file), hand_img)\n",
    "\n",
    "\n",
    "# Define paths\n",
    "source_dir = 'processed_data'\n",
    "train_dir = 'train'\n",
    "val_dir = 'val'\n",
    "\n",
    "# Create train and validation directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Split and process data\n",
    "for label in os.listdir(source_dir):\n",
    "    os.makedirs(os.path.join(train_dir, label), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, label), exist_ok=True)\n",
    "    split_data(os.path.join(source_dir, label), os.path.join(train_dir, label), os.path.join(val_dir, label),\n",
    "               split_size=0.8)\n",
    "    process_images(os.path.join(train_dir, label), os.path.join(train_dir, label))\n",
    "    process_images(os.path.join(val_dir, label), os.path.join(val_dir, label))\n",
    "\n",
    "# Set up the data generators with resized images\n",
    "img_size = (100, 100)  # Smaller image size\n",
    "batch_size = 40  # Batch size\n",
    "\n",
    "# Number of classes (replace with the actual number of classes in your dataset)\n",
    "n_classes = len(os.listdir(train_dir))\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add CNN layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))  # Additional Conv2D layer\n",
    "model.add(MaxPooling2D((2, 2)))  # Additional MaxPooling2D layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add Bidirectional LSTM layer\n",
    "model.add(Reshape((1, -1)))  # You might need to modify this depending on your dataset\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "\n",
    "# Add a dense layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // batch_size)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('model1.keras')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
